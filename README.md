# Replication materials

Replication materials for "Bias in LLMs as Annotators: The Effect of Party Cues on Labelling Decision by Large Language Models" (2024), by SebastiÃ¡n Vallejo Vera and Hunter Driggers.

> __Abstract:__
> Human coders can be biased. We test whether Large Language Models (LLMs) replicate those biases when used as text annotators. By replicating an experiment run by Ennser-Jedenastik and Meyer (2018), we find evidence that LLMs use political information, and specifically party cues, to evaluate political statements. Not only do LLMs use relevant information to contextualize whether a statement is positive, negative, or neutral based on the party cue, they also reflect the biases of the human-generated data upon which they have been trained. We also find that unlike humans, who are only biased when faced with statements from extreme parties, LLMs exhibit significant bias even when prompted with statements from center-left and center-right parties. The implications of our findings are discussed in the conclusion. 

A link to the latest pre-print is available [here](https://arxiv.org/abs/2408.15895).

This README file provides an overview of the replications materials for the article. The [Data](https://github.com/svallejovera/gender_inst_speeches#data) section describes the main dataset required to reproduce the tables and figures in the paper. The [Analysis](https://github.com/svallejovera/gender_inst_speeches#Analysis) section summarizes the purpose of each R or python script. 

## Data
  
## Analysis
